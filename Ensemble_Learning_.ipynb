{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6QTrnA0fZ3Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ensemble Learning | Assignment ***"
      ],
      "metadata": {
        "id": "Gu81DD1ufg-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer-1. Ensemble Learning is a machine learning technique that combines the predictions of multiple models to improve the overall performance and robustness of the prediction.\n",
        "\n",
        "Key Idea\n",
        "The key idea behind Ensemble Learning is to:\n",
        "\n",
        "1. Train multiple models: Train multiple base models on the same dataset.\n",
        "2. Combine predictions: Combine the predictions of these models to produce a final prediction.\n"
      ],
      "metadata": {
        "id": "2LNSjjKcflLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer-2. Bagging\n",
        "1. Bootstrap Aggregating: Bagging involves training multiple models on different subsets of the training data, created using bootstrap sampling.\n",
        "2. Independent models: Each model is trained independently, and the predictions are combined using voting or averaging.\n",
        "3. Reduces variance: Bagging helps reduce variance and overfitting by averaging the predictions of multiple models.\n",
        "\n",
        "Boosting\n",
        "1. Sequential training: Boosting involves training multiple models sequentially, with each model focusing on the errors of the previous model.\n",
        "2. Weighted data: The data is weighted based on the errors of the previous model, and the next model is trained on the weighted data.\n",
        "3. Reduces bias: Boosting helps reduce bias by iteratively improving the model's performance on the most difficult examples.\n",
        "\n",
        "Key differences\n",
        "1. Training process: Bagging trains models independently, while boosting trains models sequentially.\n",
        "2. Data weighting: Bagging uses equal weighting for all data points, while boosting uses weighted data based on errors.\n",
        "3. Error reduction: Bagging reduces variance, while boosting reduces bias.\n"
      ],
      "metadata": {
        "id": "RZ43xDwuf5RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Abswer-3. Bootstrap sampling is a statistical technique used to create multiple subsets of data from an original dataset by sampling with replacement.\n",
        "\n",
        "Role in Bagging\n",
        "1. Creating diverse subsets: Bootstrap sampling creates diverse subsets of data, which are used to train multiple models in Bagging.\n",
        "2. Reducing overfitting: By using different subsets of data, Bagging reduces overfitting and improves the generalization of the model.\n",
        "\n",
        "Role in Random Forest\n",
        "1. Decision tree training: In Random Forest, each decision tree is trained on a bootstrap sample of the data.\n",
        "2. Feature randomness: Random Forest also introduces feature randomness, where each decision tree considers a random subset of features.\n",
        "3. Combining predictions: The predictions of multiple decision trees are combined using voting or averaging to produce the final prediction.\n"
      ],
      "metadata": {
        "id": "wL86EQZ0gIsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer-4. Out-of-Bag (OOB) samples are the samples that are not included in the bootstrap sample used to train a particular model in an ensemble.\n",
        "\n",
        "OOB Score\n",
        "The OOB score is an estimate of the model's performance on unseen data, calculated using the OOB samples.\n",
        "\n",
        "Calculating OOB Score\n",
        "1. Train model on bootstrap sample: Train a model on a bootstrap sample of the data.\n",
        "2. Predict on OOB samples: Use the trained model to make predictions on the OOB samples.\n",
        "3. Calculate error: Calculate the error of the predictions on the OOB samples.\n",
        "4. Average error: Average the error across all models in the ensemble to obtain the OOB score.\n"
      ],
      "metadata": {
        "id": "TRNyC54LgjQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer-5. Single Decision Tree\n",
        "1. Feature importance: In a single Decision Tree, feature importance is calculated based on the reduction in impurity (e.g., Gini or entropy) achieved by splitting on a particular feature.\n",
        "2. Biased towards high-cardinality features: Single Decision Trees can be biased towards features with high cardinality (many unique values), which may not accurately reflect their importance.\n",
        "\n",
        "Random Forest\n",
        "1. Feature importance: In Random Forest, feature importance is calculated as the average reduction in impurity across all trees in the ensemble.\n",
        "2. More robust: Random Forest feature importance is more robust and less prone to overfitting, as it averages the importance across multiple trees.\n",
        "3. Handles high-dimensional data: Random Forest can handle high-dimensional data and provides a more accurate estimate of feature importance.\n",
        "\n",
        "Key differences\n",
        "1. Robustness: Random Forest feature importance is more robust and less prone to overfitting than a single Decision Tree.\n",
        "2. Accuracy: Random Forest feature importance is generally more accurate, especially in high-dimensional data.\n",
        "3. Interpretability: Random Forest feature importance provides a more comprehensive understanding of feature importance, as it considers the interactions between features.\n",
        "\n"
      ],
      "metadata": {
        "id": "I-Yjydt3g1aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to: ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "Answer-6."
      ],
      "metadata": {
        "id": "6yd-jJdzhEgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Get the top 5 most important features\n",
        "top_5_features = sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 most important features:\")\n",
        "for feature, importance in top_5_features:\n",
        "    print(f\"{feature}: {importance:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAIRG0NHhVeX",
        "outputId": "4e74a018-4497-4264-bb06-9f6782aeea68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area: 0.154\n",
            "worst concave points: 0.145\n",
            "mean concave points: 0.106\n",
            "worst radius: 0.078\n",
            "mean concavity: 0.068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "Answer-7."
      ],
      "metadata": {
        "id": "KZGU7YyLhc23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Accuracy of single Decision Tree:\", accuracy_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging theClassifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=100, random_state=42)\n",
        "baggingClassifier.fit(X_train, y_train)\n",
        "y_pred_bagging = baggingClassifier.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(\"Accuracy of Bagging Classifier:\", accuracy_bagging)\n",
        "\n",
        "# Compare the accuracy\n",
        "print(\"Accuracy improvement:\", accuracy_bagging - accuracy_dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "XX8mzak4hsP8",
        "outputId": "418dcf9f-2f51-4689-bb43-90b3bae556a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2361074881.py, line 24)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2361074881.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    bagging theClassifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=100, random_state=42)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy\n",
        "\n",
        "Abswer-8."
      ],
      "metadata": {
        "id": "6cHJsznihyj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Accuracy of single Decision Tree:\", accuracy_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging theClassifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=100, random_state=42)\n",
        "baggingClassifier.fit(X_train, y_train)\n",
        "y_pred_bagging = baggingClassifier.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(\"Accuracy of Bagging Classifier:\", accuracy_bagging)\n",
        "\n",
        "# Compare the accuracy\n",
        "print(\"Accuracy improvement:\", accuracy_bagging - accuracy_dt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "r30tZ7wgiGgN",
        "outputId": "fd706152-a05f-4188-9d65-358cdb1dcdf2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2089007274.py, line 24)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2089007274.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    bagging theClassifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=100, random_state=42)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "Answer-9."
      ],
      "metadata": {
        "id": "M_6EEISziMhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "cal_housing = fetch_california_housing()\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(n_estimators=100, random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_regressor.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(\"MSE of Bagging Regressor:\", mse_bagging)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(\"MSE of Random Forest Regressor:\", mse_rf)\n",
        "\n",
        "# Compare the MSE\n",
        "print(\"Difference in MSE:\", mse_bagging - mse_rf)\n",
        "if mse_bagging < mse_rf:\n",
        "    print(\"Bagging Regressor performs better\")\n",
        "elif mse_rf < mse_bagging:\n",
        "    print(\"Random Forest Regressor performs better\")\n",
        "else:\n",
        "    print(\"Both models perform equally well\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEJ13iGiiUAh",
        "outputId": "f155fe98-51c0-4e1a-ecf9-0de608d3f0f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE of Bagging Regressor: 0.25592438609899626\n",
            "MSE of Random Forest Regressor: 0.2553684927247781\n",
            "Difference in MSE: 0.000555893374218186\n",
            "Random Forest Regressor performs better\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "Answer-10.1. Choose between Bagging or Boosting\n",
        "Based on the problem and data characteristics, I would choose Boosting, specifically Gradient Boosting, due to its ability to handle complex interactions between features and its robustness to overfitting.\n",
        "\n",
        "2. Handle Overfitting\n",
        "To handle overfitting, I would use techniques such as:\n",
        "\n",
        "- Regularization: L1 or L2 regularization to prevent overfitting\n",
        "- Early stopping: Stop training when the model's performance on the validation set starts to degrade\n",
        "- Hyperparameter tuning: Tune hyperparameters such as learning rate, number of estimators, and maximum depth to find the optimal combination\n",
        "\n",
        "3. Select Base Models\n",
        "For this problem, I would use Decision Trees as the base model for the ensemble. Decision Trees are a popular choice for ensemble models due to their ability to handle complex interactions between features.\n",
        "\n",
        "4. Evaluate Performance using Cross-Validation\n",
        "To evaluate the performance of the model, I would use K-fold cross-validation with metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "5. Justify Ensemble Learning\n",
        "Ensemble learning improves decision-making in loan default prediction by providing more accurate and robust predictions. By combining the strengths of multiple models, ensemble learning can reduce the risk of loan default and enable financial institutions to make more informed decisions about lending.\n"
      ],
      "metadata": {
        "id": "4Q8f6fdNiijt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('loan_data.csv')\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X = df.drop('default', axis=1)\n",
        "y = df['default']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier (Bagging)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "\n",
        "# Train a Gradient Boosting Classifier (Boosting)\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "\n",
        "# Evaluate performance using cross-validation\n",
        "scores_rf = cross_val_score(rf, X_train, y_train, cv=5)\n",
        "scores_gb = cross_val_score(gb, X_train, y_train, cv=5)\n",
        "print(\"Random Forest Cross-Validation Accuracy:\", scores_rf.mean())\n",
        "print(\"Gradient Boosting Cross-Validation Accuracy:\", scores_gb.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "P1MLwnUZjY6w",
        "outputId": "14d82904-e3b0-41f1-e6f1-a88224a58a1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'loan_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2658587628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loan_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'loan_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "s39-sAjNjg3l"
      }
    }
  ]
}